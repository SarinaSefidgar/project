{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"Evaluating Model on test data...\")\n",
        "model.evaluate([W_test], Y_test, verbose=1)"
      ],
      "metadata": {
        "id": "FFbIqzAhujAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Evaluating Model on train data...\")\n",
        "model.evaluate([W_train], Y_train, verbose=1)"
      ],
      "metadata": {
        "id": "cu5UNOlOuVri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import gensim\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from collections import defaultdict\n",
        "from nltk.data import load\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import os\n",
        "import nltk\n",
        "import spacy\n",
        "import networkx as nx\n",
        "from tensorflow.keras.models import Sequential \n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "!pip install tensorboardcolab\n",
        "from tensorboardcolab import *\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import initializers, regularizers, constraints\n",
        "\n",
        "seed = 0\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(1)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "embedding_size=100\n",
        "vocab_size= np.shape(embedding_matrix)[0]\n",
        "embedding_dim=64\n",
        "filter_sizes=[3,4,5]\n",
        "reshape = Reshape((105,100,1))(embedding)\n",
        "reshape_sdp = Reshape((105,100,1))(short_embedding)\n",
        "conv_1 = Conv2D(64, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "\n",
        "drop1 = Dropout(0.5)(conv_1)\n",
        "max_pool1 = MaxPool2D(pool_size=(105 - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(drop1)  \n",
        "flat1 = Flatten()(max_pool1)\n",
        "\n",
        "\n",
        "conv_2 = Conv2D(64, kernel_size=(filter_sizes[2], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape)\n",
        "\n",
        "drop2 = Dropout(0.5)(conv_2)\n",
        "max_pool2 = MaxPool2D(pool_size=(105 - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(drop2)  \n",
        "flat2 = Flatten()(max_pool2)\n",
        "\n",
        "print(flat2)\n",
        "print(np.shape(embedding_matrix_sdp)[0])\n",
        "\n",
        "s_conv_1 = Conv2D(64, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_sdp)\n",
        "\n",
        "s_drop1 = Dropout(0.3)(s_conv_1)\n",
        "s_max_pool1 = MaxPool2D(pool_size=(105 - filter_sizes[0] + 1, 1), strides=(1,1), padding='valid')(s_drop1)  \n",
        "s_flat1 = Flatten()(s_max_pool1)\n",
        "\n",
        "\n",
        "s_conv_2 = Conv2D(64, kernel_size=(filter_sizes[0], embedding_dim), padding='valid', kernel_initializer='normal', activation='relu')(reshape_sdp)\n",
        "\n",
        "s_drop2 = Dropout(0.3)(s_conv_2)\n",
        "s_max_pool2 = MaxPool2D(pool_size=(105 - filter_sizes[2] + 1, 1), strides=(1,1), padding='valid')(s_drop2)  \n",
        "s_flat2 = Flatten()(s_max_pool2)\n",
        "\n",
        "\n",
        "print('khaaaaaaaar')\n",
        "concatenated_tensor = Concatenate(axis=1)([max_pool1, max_pool2])\n",
        "        \n",
        "flatten = Flatten()(concatenated_tensor)\n",
        "dropout = Dropout(0.2)(flatten)\n",
        "output = Dense(units=5, activation='softmax')(dropout)\n",
        "\n",
        "model = Model(inputs=[inputs], outputs=output)\n",
        "\n",
        "adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "\n",
        "model.compile(optimizer=adam, loss='mse', metrics=['accuracy', precision_m, recall_m, f1_m])\n",
        "model.summary()\n",
        "\n",
        "\n",
        "yt=Y_train.copy()\n",
        "yt=np.argmax(Y_train,axis=-1)\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(yt),\n",
        "                                                 yt)\n",
        "print(class_weights)\n",
        "\n",
        "model.fit([W_train], Y_train,\n",
        "                 epochs = 20, shuffle=True, batch_size=200, verbose = 1\n",
        "                 )\n"
      ],
      "metadata": {
        "id": "fMYTALeSuHdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recall_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "def precision_m(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "def f1_m(y_true, y_pred):\n",
        "    precision = precision_m(y_true, y_pred)\n",
        "    recall = recall_m(y_true, y_pred)\n",
        "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
      ],
      "metadata": {
        "id": "FFZqhoqLuFTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "K.clear_session()\n",
        "embedding_layer_sdp = tensorflow.keras.layers.Embedding(np.shape(embedding_matrix_sdp)[0],\n",
        "                            100,\n",
        "                            weights=[embedding_matrix_sdp],\n",
        "                            input_length=105,\n",
        "                            trainable=False)\n",
        "s_inputs = tensorflow.keras.layers.Input(shape=(105,), dtype='int32')\n",
        "short_embedding = embedding_layer_sdp(s_inputs)"
      ],
      "metadata": {
        "id": "PBzJaLpiuCVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = tensorflow.keras.layers.Embedding(np.shape(embedding_matrix)[0],\n",
        "                            100,\n",
        "                            weights=[embedding_matrix],\n",
        "                            input_length=105,\n",
        "                            trainable=False)\n",
        "inputs = tensorflow.keras.layers.Input(shape=(105,), dtype='int32')\n",
        "embedding = embedding_layer(inputs)\n",
        "input2=Input(shape=(sentMax,),name='d1')\n",
        "input3=Input(shape=(sentMax,),name='d2')\n",
        "input4=Input(shape=(sentMax,),name='pos')\n",
        "x2=Embedding(d1_dict_size, 10,trainable=True,input_length=sentMax)(input2)\n",
        "x3=Embedding(d2_dict_size, 10,trainable=True,input_length=sentMax)(input3)\n",
        "x4=Embedding(pos_dict_size, 4,trainable=True,input_length=sentMax)(input4)"
      ],
      "metadata": {
        "id": "cgKQz3a_t_Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix_sdp = np.zeros((len(word_index_sdp) + 1, 100))\n",
        "for word, i in word_index_sdp.items():\n",
        "    embedding_vector_sdp = embeddings_index.get(word)\n",
        "    if embedding_vector_sdp is not None:\n",
        "        embedding_matrix_sdp[i] = embedding_vector_sdp"
      ],
      "metadata": {
        "id": "75LpJkhQt7ru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_sdp=[]\n",
        "test_data_sdp=[]\n",
        "train_label_sdp=[]\n",
        "test_label_sdp=[]\n",
        "texts_train_sdp = []\n",
        "texts_test_sdp = []\n",
        "labels_sdp = []\n",
        "labels_index = {'false':0, 'advise': 1, 'mechanism': 2, 'effect': 3, 'int': 4}\n",
        "textss_sdp=[]\n",
        "\n",
        "for i in range(len(tr_sdp)):\n",
        "    sentence=tr_sdp[i]\n",
        "    textss_sdp.append(sentence)\n",
        "\n",
        "    \n",
        "\n",
        "for i in range(len(te_sdp)):\n",
        "    sentence=te_sdp[i]\n",
        "    textss_sdp.append(sentence)\n",
        "\n",
        "tokenizer  = Tokenizer(num_words = 10000)\n",
        "tokenizer.fit_on_texts(textss_sdp)\n",
        "sequences_sdp =  tokenizer.texts_to_sequences(textss_sdp)\n",
        "\n",
        "word_index_sdp = tokenizer.word_index"
      ],
      "metadata": {
        "id": "tsL-ZqNDt4Qj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_sdp=[]\n",
        "for i in train_sdp:\n",
        "  tr_sdp.append(' '.join(word for word in i))\n",
        "print(tr_sdp[0])\n",
        "\n",
        "te_sdp=[]\n",
        "for i in test_sdp:\n",
        "  te_sdp.append(' '.join(word for word in i))\n",
        "print(te_sdp[0])"
      ],
      "metadata": {
        "id": "I_9_Lxcktz0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"test_sdp.txt\", \"wb\") as fp:   #Pickling\n",
        "  pickle.dump(Te_sent_contents_SDP, fp)\n",
        "\n",
        "with open(\"test_sdp.txt\", \"rb\") as fp:   # Unpickling\n",
        "  test_sdp = pickle.load(fp)\n",
        "\n",
        "print((test_sdp[10]))\n",
        "print(len(test_sdp))"
      ],
      "metadata": {
        "id": "1PqK-pxAtulI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"train_sdp.txt\", \"wb\") as fp:   #Pickling\n",
        "  pickle.dump(Tr_sent_contents_SDP, fp)\n",
        "\n",
        "with open(\"train_sdp.txt\", \"rb\") as fp:   # Unpickling\n",
        "  train_sdp = pickle.load(fp)\n",
        "\n",
        "print(train_sdp[0])\n",
        "print(len(train_sdp))\n"
      ],
      "metadata": {
        "id": "J3SQbZaBtr59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"train_sdp.txt\", \"wb\") as fp:   #Pickling\n",
        "  pickle.dump(Tr_sent_contents_SDP, fp)\n",
        "\n",
        "with open(\"train_sdp.txt\", \"rb\") as fp:   # Unpickling\n",
        "  train_sdp = pickle.load(fp)\n",
        "\n",
        "print(train_sdp[0])\n",
        "print(len(train_sdp))\n"
      ],
      "metadata": {
        "id": "e-3BQflftpPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = np.zeros((len(word_index) + 1, 100))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "cdV3NQ_DtjXe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data=[]\n",
        "test_data=[]\n",
        "train_label=[]\n",
        "test_label=[]\n",
        "texts_train = []\n",
        "texts_test = []\n",
        "labels = []\n",
        "labels_index = {'false':0, 'advise': 1, 'mechanism': 2, 'effect': 3, 'int': 4}\n",
        "textss=[]\n",
        "\n",
        "for i in range(len(Tr_sent_contents)):\n",
        "    sentence=remove_punctuation(Tr_sent_contents[i])\n",
        "    textss.append(sentence)\n",
        "    train_label.append(label_dict[Tr_sent_lables[i]])\n",
        "    labels.append(labels_index[Tr_sent_lables[i]])\n",
        "\n",
        "for i in range(len(Te_sent_contents)):\n",
        "    sentence=remove_punctuation(Te_sent_contents[i])\n",
        "    textss.append(sentence)\n",
        "    test_label.append(label_dict[Te_sent_lables[i]])  \n",
        "    labels.append(labels_index[Te_sent_lables[i]])\n",
        "\n",
        "tokenizer  = Tokenizer(num_words = 10000)\n",
        "tokenizer.fit_on_texts(textss)\n",
        "sequences =  tokenizer.texts_to_sequences(textss)\n",
        "\n",
        "word_index = tokenizer.word_index\n"
      ],
      "metadata": {
        "id": "RA1uGfVDtes-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Activation, Conv2D, Input, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D\n",
        "from tensorflow.keras.layers import MaxPool1D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.utils import shuffle\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix"
      ],
      "metadata": {
        "id": "iEKNbd-etbZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "embeddings_index = {}\n",
        "base_path = \"\"\n",
        "filename = \"glove.6B.100d.txt\"\n",
        "path_to_file = os.path.join(base_path, filename)\n",
        "f = open(path_to_file , 'r', encoding=\"utf8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "\n",
        "def remove_punctuation(string): \n",
        "    punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "    for x in string.lower(): \n",
        "        if x in punctuations: \n",
        "            string = string.replace(x, \"\") \n",
        "    return string"
      ],
      "metadata": {
        "id": "JVCHRzwNtXSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sdp_train=mapWordToId(Tr_sent_contents_SDP, sdp_dict)\n",
        "sdp_test=mapWordToId(Te_sent_contents_SDP, sdp_dict)\n",
        "print(W_train)\n",
        "sdp_train,W_train, d1_train, d2_train,T_train, pos_train,Tr_sent_contents_SDP_expand,sdp_test,W_test, d1_test, d2_test,T_test,pos_test,Te_sent_contents_SDP_expand\\\n",
        "=paddData([sdp_train,W_train, d1_train, d2_train,T_train,pos_train,Tr_sent_contents_SDP_expand,sdp_test, W_test, d1_test, d2_test,T_test,pos_test,Te_sent_contents_SDP_expand],\n",
        "          sentMax)\n",
        "\n",
        "#print(sdp_train)"
      ],
      "metadata": {
        "id": "-lWfpiMStSTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def findSentLengths(tr_te_list):\n",
        "  lis = []\n",
        "  for lists in tr_te_list:\n",
        "    lis.append([len(l) for l in lists])\n",
        "  print(lis)\n",
        "  return lis\n",
        "\n",
        "print(Tr_word_list[1])\n",
        "train_sent_lengths,test_sent_lengths = findSentLengths([Tr_word_list,Te_word_list])\n",
        "print(train_sent_lengths)\n",
        "sentMax = max(train_sent_lengths  + test_sent_lengths)\n",
        "print (\"max sent length\", sentMax)\n",
        "sentMax=105\n",
        "\n",
        "max(test_sent_lengths)\n",
        "\n",
        "train_sent_lengths = np.array(train_sent_lengths, dtype='int32')\n",
        "test_sent_lengths = np.array(test_sent_lengths, dtype='int32')\n",
        "\n",
        "\n",
        "label_dict = {'false':0, 'advise': 1, 'mechanism': 2, 'effect': 3, 'int': 4}\n",
        "\n",
        "\n",
        "def makeDistanceList(lista):\n",
        "  sent_list = sum(lista, [])\n",
        "  wf = {}\n",
        "  for sent in sent_list:\n",
        "    for w in sent:\n",
        "      if w in wf:\n",
        "        wf[w] += 1\n",
        "      else:\n",
        "        wf[w] = 0\n",
        "   \n",
        "  wl = []\t\n",
        "  i = 1\n",
        "  for w,f in wf.items():\n",
        "    wl.append(w)\n",
        "  print(wl)\n",
        "  return wl\n",
        "\n",
        "def makeWordList(lista):  \n",
        "  sent_list = sum(lista, [])\n",
        "  print(len(sent_list))#sum train and test sentences.\n",
        "  wf = {}\n",
        "  for sent in sent_list:\n",
        "    for w in sent:\n",
        "      if w in wf:\n",
        "        wf[w] += 1\n",
        "      else:\n",
        "        wf[w] = 0\n",
        "\n",
        "  wl = []\t\n",
        "  i = 1\n",
        "\n",
        "  wl.append('<pad>')\n",
        "  wl.append('<unkown>')\n",
        "  for w,f in wf.items():\n",
        "    wl.append(w)\n",
        "  print(wf)\n",
        "  print(wl)\n",
        "  return wl\n",
        "\n",
        "\n",
        "word_dict = makeWordList([Tr_word_list, Te_word_list])\n",
        "d1_dict = makeDistanceList([Tr_d1_list,  Te_d1_list,])\n",
        "print(d1_dict)\n",
        "d2_dict = makeDistanceList([Tr_d2_list,  Te_d2_list])\n",
        "pos_dict=makeDistanceList([Tr_word_pos,Te_word_pos])\n",
        "type_dict = makeWordList([Tr_type_list, Te_type_list])\n",
        "sdp_dict=makeWordList([Tr_sent_contents_SDP, Te_sent_contents_SDP]) \n",
        "print (\"word dictonary length\", len(word_dict))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def mapLabelToId(sent_lables, label_dict):\n",
        "  if len(label_dict) > 2:\n",
        "    return [label_dict[label] for label in sent_lables]\n",
        "  else:\n",
        "    return [int (label != 'false') for label in sent_lables]\n",
        "\n",
        "def mapWordToId(sent_contents, word_list):\n",
        "  T = []\n",
        "  for sent in sent_contents:\n",
        "    t = []\n",
        "    for w in sent:\n",
        "      t.append(word_list.index(w))\n",
        "    T.append(t)\n",
        "  return T\n",
        "\n",
        "# Mapping Train\n",
        "W_train =   mapWordToId(Tr_word_list, word_dict)\n",
        "d1_train = mapWordToId(Tr_d1_list, d1_dict)\n",
        "d2_train = mapWordToId(Tr_d2_list, d2_dict)\n",
        "pos_train=mapWordToId(Tr_word_pos, pos_dict)\n",
        "T_train = mapWordToId(Tr_type_list,type_dict)\n",
        "\n",
        "\n",
        "\n",
        "#One Hot Encoding\n",
        "Y_t = mapLabelToId(Tr_sent_lables, label_dict)\n",
        "Y_train = np.zeros((len(Y_t), len(label_dict)))\n",
        "for i in range(len(Y_t)):\n",
        "\tY_train[i][Y_t[i]] = 1.0\n",
        "\n",
        "\n",
        "!# Mapping Test\n",
        "W_test =   mapWordToId(Te_word_list, word_dict)\n",
        "d1_test = mapWordToId(Te_d1_list, d1_dict)\n",
        "d2_test = mapWordToId(Te_d2_list, d2_dict)\n",
        "pos_test=mapWordToId(Te_word_pos, pos_dict)\n",
        "T_test = mapWordToId(Te_type_list,type_dict)\n",
        "\n",
        "Y_t = mapLabelToId(Te_sent_lables, label_dict)\n",
        "Y_test = np.zeros((len(Y_t), len(label_dict)))\n",
        "for i in range(len(Y_t)):\n",
        "\tY_test[i][Y_t[i]] = 1.0\n",
        "\n",
        "da=np.zeros(sentMax)\n",
        "db=np.zeros(sentMax)\n",
        "da[0]=4\n",
        "db[0]=8\n",
        "da=np.reshape(da,(1,sentMax))\n",
        "db=np.reshape(db,(1,sentMax))\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "def paddData(listL, maxl): #W_batch, d1_tatch, d2_batch, t_batch)\n",
        "  rlist = []\n",
        "  import tensorflow.keras\n",
        "  for mat in listL:\n",
        "    mat_n=tensorflow.keras.preprocessing.sequence.pad_sequences(mat, maxlen=maxl, dtype='int32', padding='post', truncating='post', value=0.0)\n",
        "    rlist.append(np.array(mat_n))\n",
        "  return rlist\n",
        "\n",
        "#padding\n",
        "W_train, d1_train, d2_train,T_train, pos_train,Tr_sent_contents_SDP_expand,W_test, d1_test, d2_test,T_test,pos_test,Te_sent_contents_SDP_expand\\\n",
        "=paddData([W_train, d1_train, d2_train,T_train,pos_train,Tr_sent_contents_SDP_expand,W_test, d1_test, d2_test,T_test,pos_test,Te_sent_contents_SDP_expand],\n",
        "          sentMax) \n",
        "            \n",
        "print (\"train\", len(W_train))\n",
        "print (\"test\", len(W_test))\n",
        "\n",
        "\n",
        "\n",
        "d1_dict_size = len(d1_dict)\n",
        "d2_dict_size = len(d2_dict)\n",
        "pos_dict_size = len(pos_dict)\n",
        "print (\"d1_dict_size\", d1_dict_size)\n",
        "print (\"d2_dict_size\", d2_dict_size)\n",
        "print (\"pos_dict_size\", pos_dict)\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)"
      ],
      "metadata": {
        "id": "Q0ZbIRvDtLrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataRead(fname):\n",
        "    print( \"Input File Reading\")\n",
        "    fp = open(fname, 'r')\n",
        "    samples = fp.read().strip().split('\\n\\n')\n",
        "    sent_lengths   = []\t\t#1-d array\n",
        "    sent_contents  = []\t\t#2-d array [[w1,w2,....] ...]\n",
        "    sent_lables    = []\t\t#1-d array\n",
        "    entity1_list   = []\t\t#2-d array [[e1,e1_t] [e1,e1_t]...]\n",
        "    entity2_list   = []\t\t#2-d array [[e1,e1_t] [e1,e1_t]...]\n",
        "    for sample in samples:\n",
        "      sent, entities, relation = sample.strip().split('\\n')\n",
        "\n",
        "      e1, e1_t, e2, e2_t = entities.split('\\t') \n",
        "      sent_contents.append(sent.lower())\n",
        "      entity1_list.append([e1, e1_t])\n",
        "      entity2_list.append([e2, e2_t])\n",
        "      sent_lables.append(relation)\n",
        "\n",
        "    return sent_contents, entity1_list, entity2_list, sent_lables\n",
        "\n",
        "ftrain = r'all_data_train_data.txt'\n",
        "ftest = r'all_data_test_data.txt'\n",
        "Tr_sent_contents, Tr_entity1_list, Tr_entity2_list, Tr_sent_lables = dataRead(ftrain)\n",
        "print(Tr_sent_contents[0])\n",
        "Te_sent_contents, Te_entity1_list, Te_entity2_list, Te_sent_lables = dataRead(ftest)\n",
        "\n",
        "train=pd.DataFrame({'sents':Tr_sent_contents})\n",
        "freq = pd.Series(' '.join(train['sents']).split()).value_counts()[-600:]\n",
        "# freq\n",
        "freq = list(freq.index)\n",
        "train['sents'] = train['sents'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "train['sents'].head()\n",
        "\n",
        "\n",
        "\n",
        "test=pd.DataFrame({'sents':Te_sent_contents})\n",
        "# \n",
        "\n",
        "freq = pd.Series(' '.join(test['sents']).split()).value_counts()[-300:]\n",
        "# freq\n",
        "freq = list(freq.index)\n",
        "test['sents'] = test['sents'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "test['sents'].head()\n",
        "\n",
        "\n",
        "Tr_sent_contents=list(train['sents'])\n",
        "print(len(Tr_sent_contents))\n",
        "Te_sent_contents=list(test['sents'])\n",
        "\n",
        "def shortest_dependency_path(sents, e1=None, e2=None):\n",
        "    temp=[]\n",
        "    for s in sents:\n",
        "      \n",
        "      doc = nlp(s)\n",
        "      edges = []\n",
        "      for token in doc:\n",
        "          for child in token.children:\n",
        "              edges.append(('{0}'.format(token),\n",
        "                            '{0}'.format(child)))\n",
        "      graph = nx.Graph(edges)\n",
        "      try:\n",
        "          shortest_path = nx.shortest_path(graph, source=e1, target=e2)\n",
        "      except:\n",
        "          shortest_path = shortest_path = [e1,e2]\n",
        "      temp.append(shortest_path)\n",
        "    return temp\n",
        "\n",
        "Tr_sent_contents_SDP=shortest_dependency_path(Tr_sent_contents, e1='druga', e2='drugb')\n",
        "print(Tr_sent_contents_SDP[0])\n",
        "Te_sent_contents_SDP=shortest_dependency_path(Te_sent_contents, e1='druga', e2='drugb')\n",
        "\n",
        "print(Tr_sent_contents_SDP[1])\n",
        "\n",
        "import re\n",
        "\n",
        "#Replace all white-space characters with the digit \"9\":\n",
        "\n",
        "txt = \"The rain in Spain\"\n",
        "x = re.sub(\"\\s\", \"9\", txt)\n",
        "print(x)\n",
        "\n",
        "def preProcess(sent):\n",
        "\tsent = sent.lower()\n",
        "\tsent = sent.replace('/',' ')\n",
        "\tsent = sent.replace('.','')\n",
        "\t#print(sent)\n",
        "\tsent = tokenizer.tokenize(sent)\n",
        "\tsent = ' '.join(sent)\n",
        "\tsent = re.sub('\\d', 'dg',sent)\n",
        "\treturn sent\n",
        "\n",
        "def makeFeatures(sent_list, entity1_list, entity2_list):\n",
        "  print ('Making Features')\n",
        "  word_list = []\n",
        "  d1_list = []\n",
        "  d2_list = []\n",
        "  type_list = []\n",
        "  for sent, ent1, ent2 in zip(sent_list, entity1_list, entity2_list):\n",
        "    sent = preProcess(sent)\n",
        "    sent_list1 = sent.split()\n",
        "    #print(sent_list1)\n",
        "    entity1 = preProcess(ent1[0]).split()\n",
        "    entity2 = preProcess(ent2[0]).split()\n",
        "    #print('entity1:' + entity1[0])\n",
        "    #print('entity2:' + entity2[0])\n",
        "    s1 = sent_list1.index('druga')\n",
        "    s2 = sent_list1.index('drugb') \n",
        "    #print(s1)\n",
        "    #print(s2)\n",
        "\t\t# distance1 feature\t\n",
        "    d1 = []\n",
        "    for i in range(len(sent_list1)):\n",
        "        if i < s1 :\n",
        "           d1.append(str(i - s1))\n",
        "        elif i > s1 :\n",
        "           d1.append(str(i - s1 ))\n",
        "        else:\n",
        "           d1.append('0')\n",
        "\t\t#distance2 feature\t\t\n",
        "    d2 = []\n",
        "    for i in range(len(sent_list1)):\n",
        "        if i < s2:\n",
        "           d2.append(str(i - s2))\n",
        "        elif i > s2:\n",
        "           d2.append(str(i - s2))\n",
        "        else:\n",
        "           d2.append('0')\n",
        "    t = []\n",
        "    for i in range(len(sent_list1)):\n",
        "      t.append('Out')\n",
        "    t[s1] = ent1[1]\t\t\n",
        "    t[s2] = ent2[1]\n",
        "\n",
        "    word_list.append(sent_list1)\n",
        "    d1_list.append(d1)\n",
        "    d2_list.append(d2)\n",
        "    type_list.append(t) \n",
        "\n",
        "  #print(sent_list1)\n",
        "  #print(d1)\n",
        "  #print(d2)\n",
        "  return word_list, d1_list, d2_list,type_list\n",
        "\n",
        "Tr_word_list, Tr_d1_list, Tr_d2_list, Tr_type_list = makeFeatures(Tr_sent_contents, Tr_entity1_list, Tr_entity2_list)\n",
        "print(Tr_word_list[0])                                                                    \n",
        "                                                                            \n",
        "\n",
        "Tr_word_pos_t=[]\n",
        "for i in Tr_word_list:\n",
        "  #print(i)\n",
        "    #print(j)\n",
        "    \n",
        "  Tr_word_pos_t.append(nltk.pos_tag(i))\n",
        "\n",
        "print(Tr_word_pos_t[0])\n",
        "\n",
        "Tr_word_pos=[]\n",
        "for i in range(np.shape(Tr_word_pos_t)[0]):\n",
        "  temp=[]\n",
        "  for j in range(np.shape(Tr_word_pos_t[i])[0]):\n",
        "    temp.append(Tr_word_pos_t[i][j][1])\n",
        "  Tr_word_pos.append(temp)\n",
        "\n",
        "print(Tr_word_pos[0])\n",
        "\n",
        "Te_word_list, Te_d1_list, Te_d2_list, Te_type_list = makeFeatures(Te_sent_contents, Te_entity1_list, Te_entity2_list)                                \n",
        "print(Te_word_list)\n",
        "Te_word_pos_t=[]\n",
        "for i in Te_word_list:\n",
        "  Te_word_pos_t.append(nltk.pos_tag(i))\n",
        "\n",
        "Te_word_pos=[]\n",
        "for i in range(np.shape(Te_word_pos_t)[0]):\n",
        "  temp=[]\n",
        "  for j in range(np.shape(Te_word_pos_t[i])[0]):\n",
        "    temp.append(Te_word_pos_t[i][j][1])\n",
        "  Te_word_pos.append(temp)\n",
        "\n",
        "def makeWordList(lista):  \n",
        "  sent_list = sum(lista, [])\n",
        "  print(len(sent_list))#sum train and test sentences.\n",
        "  wf = {}\n",
        "  for sent in sent_list:\n",
        "    for w in sent:\n",
        "      if w in wf:\n",
        "        wf[w] += 1\n",
        "      else:\n",
        "        wf[w] = 0\n",
        "\n",
        "  wl = []\t\n",
        "  i = 1\n",
        "\n",
        "  wl.append('<pad>')\n",
        "  wl.append('<unkown>')\n",
        "  for w,f in wf.items():\n",
        "    wl.append(w)\n",
        "  print(wf)\n",
        "  print(wl)\n",
        "  return wl\n",
        "\n",
        "word_dict = makeWordList([Tr_word_list, Te_word_list])\n",
        "print (\"word dictonary length\", len(word_dict))\n",
        "\n",
        "Tr_sent_contents_SDP_expand=[]\n",
        "\n",
        "for i in range(np.shape(Tr_word_list)[0]):\n",
        "  temp=[]\n",
        "  for j in range(np.shape(Tr_word_list[i])[0]):\n",
        "    \n",
        "    if Tr_word_list[i][j] in Tr_sent_contents_SDP[i]:\n",
        "      temp.append(1)\n",
        "    else:\n",
        "      temp.append(0)\n",
        "  \n",
        "  Tr_sent_contents_SDP_expand.append(temp)    \n",
        "  \n",
        "print(Tr_sent_contents_SDP_expand[0])\n",
        "\n",
        "Te_sent_contents_SDP_expand=[]\n",
        "\n",
        "for i in range(np.shape(Te_word_list)[0]):\n",
        "  temp=[]\n",
        "  for j in range(np.shape(Te_word_list[i])[0]):\n",
        "    \n",
        "    if Te_word_list[i][j] in Te_sent_contents_SDP[i]:\n",
        "      temp.append(1)\n",
        "    else:\n",
        "      temp.append(0)\n",
        "  \n",
        "  Te_sent_contents_SDP_expand.append(temp)\n",
        "print( Te_sent_contents_SDP_expand[0])"
      ],
      "metadata": {
        "id": "DGhUig4MtH-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq install -y graphviz && pip install -q pydot\n",
        "import pydot\n",
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "import operator\n",
        "def frequentWord(sents):\n",
        "  wf = {}\n",
        "  for s in sents:\n",
        "    for w in s:\n",
        "      if w in wf:\n",
        "        wf[w]+=1\n",
        "      else:\n",
        "        wf[w]=0\n",
        "\n",
        "  sorted_x = sorted(wf.items(), key=operator.itemgetter(1),reverse=True)\n",
        "  return sorted_x\n",
        "\n",
        "import scispacy\n",
        "import spacy\n",
        "\n",
        "#nlp = en_core_sci_sm.load()\n",
        "\n",
        "import networkx as nx\n",
        "import spacy\n",
        "from nltk import Tree\n",
        "nlp = spacy.load('en_core_sci_lg')\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import pandas  as pd\n",
        "import sklearn as sk\n",
        "import random\n",
        "import csv\n",
        "import re\n",
        "import collections\n",
        "import pickle\n",
        "import sys\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "wv_embSize = 100\n",
        "d1_emb_size=10\n",
        "d2_emb_size=10\n",
        "type_emb_size=10\n",
        "numfilter = 200\n",
        "LSTM_unit = 200\n",
        "num_epochs = 18\n",
        "batch_size=200\n",
        "reg_para = 0.001\n",
        "drop_out = 1.0"
      ],
      "metadata": {
        "id": "uuyS8rSZtCa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall h5py\n",
        "!pip install h5py==2.10.0"
      ],
      "metadata": {
        "id": "bi0QpuyNtALW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!pip install scispacy\n",
        "!pip3 install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_scibert-0.4.0.tar.gz\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_lg-0.4.0.tar.gz"
      ],
      "metadata": {
        "id": "fCOas4Gps9lS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IydPKD14s8Vy"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/\n",
        "!ls\n"
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#main\n",
        "K.clear_session()\n",
        "print(sentMax)#105\n",
        "input1=Input(shape=(sentMax,),name='text')\n",
        "#print(input1)\n",
        "input2=Input(shape=(sentMax,),name='d1')\n",
        "input3=Input(shape=(sentMax,),name='d2')\n",
        "input4=Input(shape=(sentMax,),name='pos')\n",
        "input5=Input(shape=(sentMax,),name='SDP_main')\n",
        "\n",
        "input5_r=Reshape((sentMax,1))(input5)\n",
        "#print(input5_r)\n",
        "print(np.shape(wv)[0])#3839\n",
        "x1=Embedding(np.shape(wv)[0], 200, weights=[wv], input_length=sentMax,trainable=True)(input1)\n",
        "print(x1)\n",
        "print(d1_dict_size)#340\n",
        "print(d2_dict_size)#336\n",
        "print(pos_dict_size)#41\n",
        "embedding_model=Model(inputs=[input1], outputs=x1)\n",
        "x2=Embedding(d1_dict_size, 10,trainable=True,input_length=sentMax)(input2)\n",
        "x3=Embedding(d2_dict_size, 10,trainable=True,input_length=sentMax)(input3)\n",
        "x4=Embedding(pos_dict_size, 4,trainable=True,input_length=sentMax)(input4)\n",
        "# x5=Dense((sentMax,1))(input5)\n",
        "inputs=concatenate([x1,x2,x3,x4,input5_r],axis=-1,name=\"concat\")\n",
        "inputs=BatchNormalization()(inputs)\n",
        "inputs=SpatialDropout1D(0.4)(inputs)\n",
        "model_h1=Bidirectional(LSTM(200,return_sequences=True,dropout=0.5,recurrent_dropout=0.5),merge_mode='concat')(inputs)\n",
        "\n",
        "att,a=AttentionWitPositionAndSimilarity_v2()([model_h1,x2,x3])\n",
        "\n",
        "att=BatchNormalization()(att)\n",
        "main_output=Dropout(0.5, noise_shape=None, seed=None)(att)   \n",
        "\n",
        "main_output=Dense(300,kernel_regularizer=regularizers.l2(0.001))(main_output)            \n",
        "main_output=BatchNormalization()(main_output)\n",
        "main_output=LeakyReLU()(main_output)\n",
        "\n",
        "main_output=Dropout(0.5, noise_shape=None, seed=None)(main_output)  \n",
        "main_output = Dense(100,kernel_regularizer=regularizers.l2(0.001))(main_output)\n",
        "main_output=BatchNormalization()(main_output)\n",
        "main_output=LeakyReLU()(main_output)\n",
        "\n",
        "main_output = Dense(5,activation='softmax',kernel_regularizer=regularizers.l2(0.0001))(main_output)\n",
        "\n",
        "# ,input1_h1,input2_h1,input3_h1,input4_h1,input_sdp\n",
        "output_model = Model(inputs=[input1,input2,input3,input4,input5], outputs=main_output)\n",
        "\n",
        "sgd = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0, nesterov=True)\n",
        "# adam=Adam(lr=0.001,  beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0, amsgrad=False)\n",
        "output_model.compile(loss = 'categorical_crossentropy', optimizer=sgd,metrics=[f1])\n",
        "##78.3\n",
        "\n",
        "output_model.summary()\n",
        "\n",
        "from keras.utils.vis_utils import plot_model as plot\n",
        "plot(output_model, to_file='./model.png', show_shapes=True)\n",
        "from IPython.display import Image\n",
        "Image('./model.png')\n",
        "\n",
        "\n",
        "yt=Y_train.copy()\n",
        "yt=np.argmax(Y_train,axis=-1)\n",
        "from sklearn.utils import class_weight\n",
        "class_weights = class_weight.compute_class_weight('balanced',\n",
        "                                                 np.unique(yt),\n",
        "                                                 yt)\n",
        "print(class_weights)\n",
        "\n",
        "output_model.fit([W_train,d1_train,d2_train,pos_train,Tr_sent_contents_SDP_expand], Y_train,\n",
        "                 epochs = 150, shuffle=True, batch_size=200, verbose = 1,\n",
        "                 validation_data=([W_test, d1_test,d2_test,pos_test,Te_sent_contents_SDP_expand],Y_test),class_weight=class_weights)"
      ],
      "metadata": {
        "id": "1n8VTFMookVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import backend as K, initializers, regularizers, constraints\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "from keras.models import Sequential \n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras import regularizers\n",
        "import keras\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "!pip install tensorboardcolab\n",
        "from tensorboardcolab import *\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "\n",
        "from keras import initializers, regularizers, constraints\n",
        "\n",
        "\n",
        "\n",
        "from keras import backend as K, initializers, regularizers, constraints\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class AttentionWitPositionAndSimilarity_v2(Layer):\n",
        "      def __init__(self,\n",
        "                 W_regularizer=regularizers.l2(0.0001), b_regularizer=None,\n",
        "                 W_constraint=None, b_constraint=None,\n",
        "                 bias=True,\n",
        "                 return_attention=True,\n",
        "                 **kwargs):\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.return_attention = return_attention\n",
        "        self.init = initializers.get('glorot_uniform')\n",
        "\n",
        "        self.W_regularizer = regularizers.get(W_regularizer)\n",
        "        self.b_regularizer = regularizers.get(b_regularizer)\n",
        "\n",
        "        self.W_constraint = constraints.get(W_constraint)\n",
        "        self.b_constraint = constraints.get(b_constraint)\n",
        "\n",
        "        self.bias = bias\n",
        "        super(AttentionWitPositionAndSimilarity_v2, self).__init__(**kwargs)\n",
        "      def build(self, input_shape):\n",
        "        assert len(input_shape) == 3\n",
        "        \n",
        "\n",
        "        \n",
        "        self.W = self.add_weight(name='{}_W'.format(self.name),\n",
        "                                 shape=(input_shape[0][-1],input_shape[0][-1]),\n",
        "                                 initializer=self.init,\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "       \n",
        "        \n",
        "        self.WP1 = self.add_weight(name='{}_WP1'.format(self.name),\n",
        "                                  shape=(10,input_shape[0][-1]),\n",
        "                                 initializer=self.init,\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "\n",
        "        self.WP2 = self.add_weight(name='{}_WP2'.format(self.name),\n",
        "                                   shape=(10,input_shape[0][-1]),\n",
        "                                 initializer=self.init,\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        \n",
        "        self.WS = self.add_weight(name='{}_WS'.format(self.name),\n",
        "                                  shape=(1,input_shape[0][-1]),\n",
        "                                 initializer=self.init,\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)          \n",
        "        \n",
        "        self.v = self.add_weight(name='{}_v'.format(self.name),\n",
        "                                 shape=(input_shape[0][-1],1),\n",
        "                                 initializer=self.init,\n",
        "                                 regularizer=self.W_regularizer,\n",
        "                                 constraint=self.W_constraint)\n",
        "        if self.bias:\n",
        "            self.b = self.add_weight(name='{}_b'.format(self.name),\n",
        "                                     shape=(input_shape[0][1],input_shape[0][-1]),\n",
        "                                     initializer='zero',\n",
        "                                     regularizer=self.b_regularizer,\n",
        "                                     constraint=self.b_constraint)\n",
        "        else:\n",
        "            self.b = None\n",
        "\n",
        "        self.built = True\n",
        "\n",
        "\n",
        "      def compute_mask(self, input, input_mask=None):\n",
        "        # do not pass the mask to the next layers\n",
        "          return None\n",
        "\n",
        "      def call(self, x, mask=None):\n",
        "        \n",
        "        d1_emb=embedding_model.predict([da])[0][0]\n",
        "        d2_emb=embedding_model.predict([db])[0][0]\n",
        "        \n",
        "        d1 =  K.variable(d1_emb, dtype='float32',name=\"input_d1\")\n",
        "        d1=K.expand_dims(d1,axis=-1)\n",
        "        d1_sco=K.dot(x1,d1)\n",
        "        d1_sco=d1_sco/wv_embSize\n",
        "        d1_sco_soft=keras.activations.softmax(d1_sco, axis=1)\n",
        "\n",
        "        d2 =  K.variable(d2_emb, dtype='float32',name=\"input_d2\")\n",
        "        d2=K.expand_dims(d2,axis=-1)\n",
        "        d2_sco=K.dot(x1,d2)\n",
        "        d2_sco=d2_sco/wv_embSize\n",
        "        d2_sco_soft=keras.activations.softmax(d2_sco, axis=1)\n",
        "\n",
        "        avg=(d1_sco+d2_sco)/2#(?,143,1)\n",
        "        \n",
        "\n",
        "        \n",
        "        ew = K.dot(x[0], self.W)\n",
        "        ewp1=K.dot(x[1],self.WP1)\n",
        "        ewp2=K.dot(x[2],self.WP2)\n",
        "        ews=K.dot(avg,self.WS)\n",
        "\n",
        "        avg_all=(ewp1+ewp2+ews)/3\n",
        "        avg_all=keras.activations.softmax(avg_all, axis=1)\n",
        "        eij=ew+avg_all\n",
        "        eij += self.b\n",
        "\n",
        "        eij = K.tanh(eij)\n",
        "        eij=K.dot(eij,self.v)\n",
        "        eij=K.squeeze(eij,axis=-1)\n",
        "\n",
        "        a = K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
        "             a *= K.cast(mask, K.floatx())\n",
        "\n",
        "\n",
        "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
        "\n",
        "        weighted_input = x[0] * K.expand_dims(a)\n",
        "\n",
        "        result = K.sum(weighted_input, axis=1)\n",
        "\n",
        "        if self.return_attention:\n",
        "            return [result, a]\n",
        "        return result\n",
        "\n",
        "      def compute_output_shape(self, input_shape):\n",
        "        if self.return_attention:\n",
        "            return [(input_shape[0][0], input_shape[0][-1]),\n",
        "                    (input_shape[0][0], input_shape[0][1])]\n",
        "        else:\n",
        "            return input_shape[0][0], input_shape[0][-1]"
      ],
      "metadata": {
        "id": "HK5FgouuodCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataRead(fname):\n",
        "    print( \"Input File Reading\")\n",
        "    fp = open(fname, 'r')\n",
        "    samples = fp.read().strip().split('\\n\\n')\n",
        "    sent_lengths   = []\t\t#1-d array\n",
        "    sent_contents  = []\t\t#2-d array [[w1,w2,....] ...]\n",
        "    sent_lables    = []\t\t#1-d array\n",
        "    entity1_list   = []\t\t#2-d array [[e1,e1_t] [e1,e1_t]...]\n",
        "    entity2_list   = []\t\t#2-d array [[e1,e1_t] [e1,e1_t]...]\n",
        "    for sample in samples:\n",
        "      sent, entities, relation = sample.strip().split('\\n')\n",
        "\n",
        "      e1, e1_t, e2, e2_t = entities.split('\\t') \n",
        "      sent_contents.append(sent.lower())\n",
        "      entity1_list.append([e1, e1_t])\n",
        "      entity2_list.append([e2, e2_t])\n",
        "      sent_lables.append(relation)\n",
        "\n",
        "    return sent_contents, entity1_list, entity2_list, sent_lables\n",
        "\n",
        "ftrain = r'all_data_train_data.txt'\n",
        "ftest = r'all_data_test_data.txt'\n",
        "Tr_sent_contents, Tr_entity1_list, Tr_entity2_list, Tr_sent_lables = dataRead(ftrain)\n",
        "print(Tr_sent_contents[0])\n",
        "Te_sent_contents, Te_entity1_list, Te_entity2_list, Te_sent_lables = dataRead(ftest)\n",
        "\n",
        "train=pd.DataFrame({'sents':Tr_sent_contents})\n",
        "freq = pd.Series(' '.join(train['sents']).split()).value_counts()[-600:]\n",
        "# freq\n",
        "freq = list(freq.index)\n",
        "train['sents'] = train['sents'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "train['sents'].head()\n",
        "\n",
        "\n",
        "\n",
        "test=pd.DataFrame({'sents':Te_sent_contents})\n",
        "# \n",
        "\n",
        "freq = pd.Series(' '.join(test['sents']).split()).value_counts()[-300:]\n",
        "# freq\n",
        "freq = list(freq.index)\n",
        "test['sents'] = test['sents'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "test['sents'].head()\n",
        "\n",
        "\n",
        "Tr_sent_contents=list(train['sents'])\n",
        "print(len(Tr_sent_contents))\n",
        "Te_sent_contents=list(test['sents'])\n",
        "\n",
        "def shortest_dependency_path(sents, e1=None, e2=None):\n",
        "    temp=[]\n",
        "    for s in sents:\n",
        "      \n",
        "      doc = nlp(s)\n",
        "      edges = []\n",
        "      for token in doc:\n",
        "          for child in token.children:\n",
        "              edges.append(('{0}'.format(token),\n",
        "                            '{0}'.format(child)))\n",
        "      graph = nx.Graph(edges)\n",
        "      try:\n",
        "          shortest_path = nx.shortest_path(graph, source=e1, target=e2)\n",
        "      except:\n",
        "          shortest_path = []\n",
        "      temp.append(shortest_path)\n",
        "    return temp\n",
        "\n",
        "Tr_sent_contents_SDP=shortest_dependency_path(Tr_sent_contents, e1='druga', e2='drugb')\n",
        "print(Tr_sent_contents_SDP[0])\n",
        "Te_sent_contents_SDP=shortest_dependency_path(Te_sent_contents, e1='druga', e2='drugb')\n",
        "\n",
        "print(Tr_sent_contents_SDP[1])\n",
        "\n",
        "import re\n",
        "\n",
        "#Replace all white-space characters with the digit \"9\":\n",
        "\n",
        "txt = \"The rain in Spain\"\n",
        "x = re.sub(\"\\s\", \"9\", txt)\n",
        "print(x)\n",
        "\n",
        "def preProcess(sent):\n",
        "\tsent = sent.lower()\n",
        "\tsent = sent.replace('/',' ')\n",
        "\tsent = sent.replace('.','')\n",
        "\t#print(sent)\n",
        "\tsent = tokenizer.tokenize(sent)\n",
        "\tsent = ' '.join(sent)\n",
        "\tsent = re.sub('\\d', 'dg',sent)\n",
        "\treturn sent\n",
        "\n",
        "def makeFeatures(sent_list, entity1_list, entity2_list):\n",
        "  print ('Making Features')\n",
        "  word_list = []\n",
        "  d1_list = []\n",
        "  d2_list = []\n",
        "  type_list = []\n",
        "  for sent, ent1, ent2 in zip(sent_list, entity1_list, entity2_list):\n",
        "    sent = preProcess(sent)\n",
        "    sent_list1 = sent.split()\n",
        "    #print(sent_list1)\n",
        "    entity1 = preProcess(ent1[0]).split()\n",
        "    entity2 = preProcess(ent2[0]).split()\n",
        "    #print('entity1:' + entity1[0])\n",
        "    #print('entity2:' + entity2[0])\n",
        "    s1 = sent_list1.index('druga')\n",
        "    s2 = sent_list1.index('drugb') \n",
        "    #print(s1)\n",
        "    #print(s2)\n",
        "\t\t# distance1 feature\t\n",
        "    d1 = []\n",
        "    for i in range(len(sent_list1)):\n",
        "        if i < s1 :\n",
        "           d1.append(str(i - s1))\n",
        "        elif i > s1 :\n",
        "           d1.append(str(i - s1 ))\n",
        "        else:\n",
        "           d1.append('0')\n",
        "\t\t#distance2 feature\t\t\n",
        "    d2 = []\n",
        "    for i in range(len(sent_list1)):\n",
        "        if i < s2:\n",
        "           d2.append(str(i - s2))\n",
        "        elif i > s2:\n",
        "           d2.append(str(i - s2))\n",
        "        else:\n",
        "           d2.append('0')\n",
        "    t = []\n",
        "    for i in range(len(sent_list1)):\n",
        "      t.append('Out')\n",
        "    t[s1] = ent1[1]\t\t\n",
        "    t[s2] = ent2[1]\n",
        "\n",
        "    word_list.append(sent_list1)\n",
        "    d1_list.append(d1)\n",
        "    d2_list.append(d2)\n",
        "    type_list.append(t) \n",
        "\n",
        "  #print(sent_list1)\n",
        "  #print(d1)\n",
        "  #print(d2)\n",
        "  return word_list, d1_list, d2_list,type_list\n",
        "\n",
        "Tr_word_list, Tr_d1_list, Tr_d2_list, Tr_type_list = makeFeatures(Tr_sent_contents, Tr_entity1_list, Tr_entity2_list)\n",
        "print(Tr_word_list[0])                                                                    \n",
        "                                                                            \n",
        "\n",
        "Tr_word_pos_t=[]\n",
        "for i in Tr_word_list:\n",
        "  #print(i)\n",
        "    #print(j)\n",
        "    \n",
        "  Tr_word_pos_t.append(nltk.pos_tag(i))\n",
        "\n",
        "print(Tr_word_pos_t[0])\n",
        "\n",
        "Tr_word_pos=[]\n",
        "for i in range(np.shape(Tr_word_pos_t)[0]):\n",
        "  temp=[]\n",
        "  for j in range(np.shape(Tr_word_pos_t[i])[0]):\n",
        "    temp.append(Tr_word_pos_t[i][j][1])\n",
        "  Tr_word_pos.append(temp)\n",
        "\n",
        "print(Tr_word_pos[0])\n",
        "\n",
        "Te_word_list, Te_d1_list, Te_d2_list, Te_type_list = makeFeatures(Te_sent_contents, Te_entity1_list, Te_entity2_list)                                \n",
        "print(Te_word_list)\n",
        "Te_word_pos_t=[]\n",
        "for i in Te_word_list:\n",
        "  Te_word_pos_t.append(nltk.pos_tag(i))\n",
        "\n",
        "Te_word_pos=[]\n",
        "for i in range(np.shape(Te_word_pos_t)[0]):\n",
        "  temp=[]\n",
        "  for j in range(np.shape(Te_word_pos_t[i])[0]):\n",
        "    temp.append(Te_word_pos_t[i][j][1])\n",
        "  Te_word_pos.append(temp)\n",
        "\n",
        "def makeWordList(lista):  \n",
        "  sent_list = sum(lista, [])\n",
        "  print(len(sent_list))#sum train and test sentences.\n",
        "  wf = {}\n",
        "  for sent in sent_list:\n",
        "    for w in sent:\n",
        "      if w in wf:\n",
        "        wf[w] += 1\n",
        "      else:\n",
        "        wf[w] = 0\n",
        "\n",
        "  wl = []\t\n",
        "  i = 1\n",
        "\n",
        "  wl.append('<pad>')\n",
        "  wl.append('<unkown>')\n",
        "  for w,f in wf.items():\n",
        "    wl.append(w)\n",
        "  print(wf)\n",
        "  print(wl)\n",
        "  return wl\n",
        "\n",
        "word_dict = makeWordList([Tr_word_list, Te_word_list])\n",
        "print (\"word dictonary length\", len(word_dict))\n",
        "\n",
        "Tr_sent_contents_SDP_expand=[]\n",
        "\n",
        "for i in range(np.shape(Tr_word_list)[0]):\n",
        "  temp=[]\n",
        "  for j in range(np.shape(Tr_word_list[i])[0]):\n",
        "    \n",
        "    if Tr_word_list[i][j] in Tr_sent_contents_SDP[i]:\n",
        "      temp.append(1)\n",
        "    else:\n",
        "      temp.append(0)\n",
        "  \n",
        "  Tr_sent_contents_SDP_expand.append(temp)    \n",
        "  \n",
        "print(Tr_sent_contents_SDP_expand[0])\n",
        "\n",
        "Te_sent_contents_SDP_expand=[]\n",
        "\n",
        "for i in range(np.shape(Te_word_list)[0]):\n",
        "  temp=[]\n",
        "  for j in range(np.shape(Te_word_list[i])[0]):\n",
        "    \n",
        "    if Te_word_list[i][j] in Te_sent_contents_SDP[i]:\n",
        "      temp.append(1)\n",
        "    else:\n",
        "      temp.append(0)\n",
        "  \n",
        "  Te_sent_contents_SDP_expand.append(temp)\n",
        "print( Te_sent_contents_SDP_expand[0])\n",
        "\n",
        "def findSentLengths(tr_te_list):\n",
        "  lis = []\n",
        "  for lists in tr_te_list:\n",
        "    lis.append([len(l) for l in lists])\n",
        "  print(lis)\n",
        "  return lis\n",
        "\n",
        "print(Tr_word_list[1])\n",
        "train_sent_lengths,test_sent_lengths = findSentLengths([Tr_word_list,Te_word_list])\n",
        "print(train_sent_lengths)\n",
        "sentMax = max(train_sent_lengths  + test_sent_lengths)\n",
        "print (\"max sent length\", sentMax)\n",
        "sentMax=105\n",
        "\n",
        "max(test_sent_lengths)\n",
        "\n",
        "train_sent_lengths = np.array(train_sent_lengths, dtype='int32')\n",
        "test_sent_lengths = np.array(test_sent_lengths, dtype='int32')\n",
        "\n",
        "\n",
        "label_dict = {'false':0, 'advise': 1, 'mechanism': 2, 'effect': 3, 'int': 4}\n",
        "\n",
        "\n",
        "def makeDistanceList(lista):\n",
        "  sent_list = sum(lista, [])\n",
        "  wf = {}\n",
        "  for sent in sent_list:\n",
        "    for w in sent:\n",
        "      if w in wf:\n",
        "        wf[w] += 1\n",
        "      else:\n",
        "        wf[w] = 0\n",
        "   \n",
        "  wl = []\t\n",
        "  i = 1\n",
        "  for w,f in wf.items():\n",
        "    wl.append(w)\n",
        "  print(wl)\n",
        "  return wl\n",
        "\n",
        "def makeWordList(lista):  \n",
        "  sent_list = sum(lista, [])\n",
        "  print(len(sent_list))#sum train and test sentences.\n",
        "  wf = {}\n",
        "  for sent in sent_list:\n",
        "    for w in sent:\n",
        "      if w in wf:\n",
        "        wf[w] += 1\n",
        "      else:\n",
        "        wf[w] = 0\n",
        "\n",
        "  wl = []\t\n",
        "  i = 1\n",
        "\n",
        "  wl.append('<pad>')\n",
        "  wl.append('<unkown>')\n",
        "  for w,f in wf.items():\n",
        "    wl.append(w)\n",
        "  print(wf)\n",
        "  print(wl)\n",
        "  return wl\n",
        "\n",
        "\n",
        "word_dict = makeWordList([Tr_word_list, Te_word_list])\n",
        "d1_dict = makeDistanceList([Tr_d1_list,  Te_d1_list,])\n",
        "d2_dict = makeDistanceList([Tr_d2_list,  Te_d2_list])\n",
        "pos_dict=makeDistanceList([Tr_word_pos,Te_word_pos])\n",
        "type_dict = makeWordList([Tr_type_list, Te_type_list])\n",
        "\n",
        "print (\"word dictonary length\", len(word_dict))\n",
        "\n",
        "!pip install gensim\n",
        "from gensim.models import KeyedVectors\n",
        "filename = r'wikipedia-pubmed-and-PMC-w2v.bin'\n",
        "WV_model = KeyedVectors.load_word2vec_format(filename, binary=True)\n",
        "\n",
        "def readWordEmb(word_list, fname, embSize):\n",
        "  print (\"Reading word vectors\")\n",
        "  wv = []\n",
        "  wl = []\n",
        "  \n",
        "  for word in fname.vocab:\n",
        "    wv.append(fname[word])\n",
        "    wl.append(word)\n",
        "  wordemb = []\n",
        "  count = 0\n",
        "  for word in word_list:\n",
        "    if word in wl:\n",
        "      wordemb.append(wv[wl.index(word)])\n",
        "    else:\n",
        "      print(word)\n",
        "      count += 1\n",
        "      wordemb.append(np.random.rand(embSize))\n",
        "      \n",
        "  wordemb[word_list.index('<pad>')] = np.zeros(embSize)\n",
        "  wordemb = np.asarray(wordemb, dtype='object')\n",
        "      \n",
        "  print (\"number of unknown word in word embedding\", count)\n",
        "  print(len(wordemb))\n",
        "  print(wordemb)\n",
        "  return wordemb\n",
        "\n",
        "# # # Word Embedding\n",
        "wv = readWordEmb(word_dict, WV_model, wv_embSize)\n",
        "\n",
        "def mapLabelToId(sent_lables, label_dict):\n",
        "  if len(label_dict) > 2:\n",
        "    return [label_dict[label] for label in sent_lables]\n",
        "  else:\n",
        "    return [int (label != 'false') for label in sent_lables]\n",
        "\n",
        "def mapWordToId(sent_contents, word_list):\n",
        "  T = []\n",
        "  for sent in sent_contents:\n",
        "    t = []\n",
        "    for w in sent:\n",
        "      t.append(word_list.index(w))\n",
        "    T.append(t)\n",
        "  return T\n",
        "\n",
        "# Mapping Train\n",
        "W_train =   mapWordToId(Tr_word_list, word_dict)\n",
        "d1_train = mapWordToId(Tr_d1_list, d1_dict)\n",
        "d2_train = mapWordToId(Tr_d2_list, d2_dict)\n",
        "pos_train=mapWordToId(Tr_word_pos, pos_dict)\n",
        "T_train = mapWordToId(Tr_type_list,type_dict)\n",
        "#One Hot Encoding\n",
        "Y_t = mapLabelToId(Tr_sent_lables, label_dict)\n",
        "Y_train = np.zeros((len(Y_t), len(label_dict)))\n",
        "for i in range(len(Y_t)):\n",
        "\tY_train[i][Y_t[i]] = 1.0\n",
        "\n",
        "\n",
        "!# Mapping Test\n",
        "W_test =   mapWordToId(Te_word_list, word_dict)\n",
        "d1_test = mapWordToId(Te_d1_list, d1_dict)\n",
        "d2_test = mapWordToId(Te_d2_list, d2_dict)\n",
        "pos_test=mapWordToId(Te_word_pos, pos_dict)\n",
        "T_test = mapWordToId(Te_type_list,type_dict)\n",
        "\n",
        "Y_t = mapLabelToId(Te_sent_lables, label_dict)\n",
        "Y_test = np.zeros((len(Y_t), len(label_dict)))\n",
        "for i in range(len(Y_t)):\n",
        "\tY_test[i][Y_t[i]] = 1.0\n",
        "\n",
        "da=np.zeros(sentMax)\n",
        "db=np.zeros(sentMax)\n",
        "da[0]=4\n",
        "db[0]=8\n",
        "da=np.reshape(da,(1,sentMax))\n",
        "db=np.reshape(db,(1,sentMax))\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "def paddData(listL, maxl): #W_batch, d1_tatch, d2_batch, t_batch)\n",
        "  rlist = []\n",
        "  import keras\n",
        "  for mat in listL:\n",
        "    mat_n=keras.preprocessing.sequence.pad_sequences(mat, maxlen=maxl, dtype='int32', padding='post', truncating='post', value=0.0)\n",
        "    rlist.append(np.array(mat_n))\n",
        "  return rlist\n",
        "\n",
        "#padding\n",
        "W_train, d1_train, d2_train,T_train, pos_train,Tr_sent_contents_SDP_expand,W_test, d1_test, d2_test,T_test,pos_test,Te_sent_contents_SDP_expand\\\n",
        "=paddData([W_train, d1_train, d2_train,T_train,pos_train,Tr_sent_contents_SDP_expand,W_test, d1_test, d2_test,T_test,pos_test,Te_sent_contents_SDP_expand],\n",
        "          sentMax) \n",
        "            \n",
        "print (\"train\", len(W_train))\n",
        "print (\"test\", len(W_test))\n",
        "\n",
        "\n",
        "#vocabulary size\n",
        "word_dict_size = len(wv)\n",
        "d1_dict_size = len(d1_dict)\n",
        "d2_dict_size = len(d2_dict)\n",
        "pos_dict_size = len(pos_dict)\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras.backend as K\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
        "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2*p*r / (p+r+K.epsilon())\n",
        "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)"
      ],
      "metadata": {
        "id": "t49dLveSoYWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://evexdb.org/pmresources/vec-space-models/wikipedia-pubmed-and-PMC-w2v.bin"
      ],
      "metadata": {
        "id": "AiDM4O7OoVlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import networkx as nx\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(u'Convulsions that occur after DTaP are caused by a fever.')\n",
        "for token in doc:\n",
        "    print((token.head.text, token.text, token.dep_))"
      ],
      "metadata": {
        "id": "I1NXzPRWoSMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "!pip install spacy==2.3.5\n",
        "\n",
        "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz\n"
      ],
      "metadata": {
        "id": "rVkiMb0ToNcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get -qq install -y graphviz && pip install -q pydot\n",
        "import pydot\n",
        "import nltk\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "tokenizer = WordPunctTokenizer()\n",
        "import operator\n",
        "def frequentWord(sents):\n",
        "  wf = {}\n",
        "  for s in sents:\n",
        "    for w in s:\n",
        "      if w in wf:\n",
        "        wf[w]+=1\n",
        "      else:\n",
        "        wf[w]=0\n",
        "\n",
        "  sorted_x = sorted(wf.items(), key=operator.itemgetter(1),reverse=True)\n",
        "  return sorted_x\n",
        "\n",
        "import scispacy\n",
        "import spacy\n",
        "\n",
        "#nlp = en_core_sci_sm.load()\n",
        "\n",
        "import networkx as nx\n",
        "import spacy\n",
        "from nltk import Tree\n",
        "nlp = spacy.load('en_core_sci_lg')\n",
        "\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import pandas  as pd\n",
        "import sklearn as sk\n",
        "import random\n",
        "import csv\n",
        "import re\n",
        "import collections\n",
        "import pickle\n",
        "import sys\n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "wv_embSize = 200\n",
        "d1_emb_size=10\n",
        "d2_emb_size=10\n",
        "type_emb_size=10\n",
        "numfilter = 200\n",
        "LSTM_unit = 200\n",
        "num_epochs = 18\n",
        "batch_size=200\n",
        "reg_para = 0.001\n",
        "drop_out = 1.0"
      ],
      "metadata": {
        "id": "0qEdX_2MoICp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!pip install scispacy\n",
        "!pip3 install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_scibert-0.4.0.tar.gz\n",
        "!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_lg-0.4.0.tar.gz"
      ],
      "metadata": {
        "id": "C-GZdEYGoEdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/\n",
        "!ls"
      ],
      "metadata": {
        "id": "YsAygxiRoBD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==2.5.0 \n",
        "%tensorflow_version 1.x"
      ],
      "metadata": {
        "id": "d-Jd0EwEoApD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}